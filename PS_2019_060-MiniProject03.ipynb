{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNhud3Am4g0kPtXWpsO1WTL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## **English to Sinhala Translation with Transformers**"],"metadata":{"id":"kwNt4TUlN7L2"}},{"cell_type":"markdown","source":["## **Necessary Library Imports**"],"metadata":{"id":"zY95CJ-oOWY4"}},{"cell_type":"code","execution_count":20,"metadata":{"id":"fYojLvzINxA4","executionInfo":{"status":"ok","timestamp":1713683484136,"user_tz":-330,"elapsed":3,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}}},"outputs":[],"source":["import random\n","import tensorflow as tf\n","import string\n","import re\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","source":["## **Prepare the Data**"],"metadata":{"id":"R_ALP9nqOaf0"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4rY4NUhtOdgM","executionInfo":{"status":"ok","timestamp":1713683492030,"user_tz":-330,"elapsed":4386,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}},"outputId":"9bc3823c-2179-473c-d3b3-914d23718cbc"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## **Read the data file**"],"metadata":{"id":"lKfnlZymOk8M"}},{"cell_type":"code","source":["text_file = \"/content/drive/My Drive/dataset/EnglishSinhalaTranslateDataset.txt\"\n","with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","i = 0\n","for line in lines:\n","  print(line)\n","  i = i + 1\n","  if(i==20):\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mio1R6B0OohO","executionInfo":{"status":"ok","timestamp":1713683497786,"user_tz":-330,"elapsed":706,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}},"outputId":"c72a8009-ff8a-44ff-afc7-13a677728b8f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Go.\tයන්න.\n","Hi.\tආයුබෝවන්.\n","Run.\tදුවන්න.\n","Who?\tකව්ද?\n","Wow!\tවාව්!\n","Fire.\tගින්නක්.\n","Help.\tඋදව්.\n","Hide.\tසඟවන්න.\n","Jump.\tපනින්න.\n","Stay.\tරැඳී සිටින්න.\n","Stop.\tනවත්වන්න.\n","Wait.\tඉන්න.\n","Begin.\tආරම්භය.\n","Go on.\tදිගටම යන්න.\n","Hello!\tහෙලෝ!\n","Hurry!\tඉක්මන් කරන්න!\n","I hid.\tමම සැඟවී සිටියෙමි.\n","I ran.\tමම දිව්වා.\n","I try.\tමම උත්සාහ කරමි.\n","I won.\tමම දිනුවා.\n"]}]},{"cell_type":"code","source":["for x in range(len(lines)-10,len(lines)):\n","  print(lines[x])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MQVnGJMxO4OF","executionInfo":{"status":"ok","timestamp":1713683504434,"user_tz":-330,"elapsed":9,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}},"outputId":"481472eb-a04b-4b76-fa4b-c3c3e1c495de"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\tචොක්ලට් යනු වැනිලා බව මිනිසුන්ට ඒත්තු ගැන්වීමට ඔබ කොතරම් උත්සාහ කළත්, එය වැනිලා බව ඔබට සහ තවත් කිහිප දෙනෙකුට ඒත්තු ගැන්විය හැකි වුවද, එය තවමත් චොකලට් වනු ඇත.\n","In 1969, Roger Miller recorded a song called \"You Don't Want My Love.\" Today, this song is better known as \"In the Summer Time.\" It's the first song he wrote and sang that became popular.\t1969 දී රොජර් මිලර් \"ඔබට මගේ ආදරය අවශ්‍ය නැත\" නමින් ගීතයක් පටිගත කළේය. අද මෙම ගීතය වඩාත් ප්‍රචලිත වන්නේ \"ගිම්හානයේ\" යනුවෙනි. එය ඔහු ලියූ සහ ගායනා කළ පළමු ගීතය ජනප්‍රිය විය.\n","A child who is a native speaker usually knows many things about his or her language that a non-native speaker who has been studying for years still does not know and perhaps will never know.\tස්වදේශික කථිකයෙකු වන දරුවෙකු සාමාන්‍යයෙන් ඔහුගේ හෝ ඇයගේ භාෂාව පිළිබඳ බොහෝ දේ දන්නා අතර එය වසර ගණනාවක් තිස්සේ අධ්‍යයනය කරන ස්වදේශික නොවන කථිකයෙකු තවමත් නොදන්නා සහ කිසි විටෙකත් නොදැන සිටිය හැකිය.\n","There are four main causes of alcohol-related death. Injury from car accidents or violence is one. Diseases like cirrhosis of the liver, cancer, heart and blood system diseases are the others.\tමත්පැන් නිසා සිදුවන මරණවලට ප්‍රධාන හේතු හතරක් තිබේ. රිය අනතුරකින් හෝ ප්‍රචණ්ඩත්වයකින් තුවාල වීම එකකි. අක්මාවේ සිරෝසිස්, පිළිකා, හෘද රෝග සහ රුධිර සංසරණ පද්ධතිය වැනි රෝග අනෙක් ඒවා වේ.\n","There are mothers and fathers who will lie awake after the children fall asleep and wonder how they'll make the mortgage, or pay their doctor's bills, or save enough for their child's college education.\tඋකස් හෝ දොස්තරගේ බිල් ගෙවන්නේ කෙසේද, දරුවන්ගේ විශ්වවිද්‍යාල අධ්‍යාපනයට ප්‍රමාණවත් මුදලක් ඉතිරි කරන්නේ කෙසේදැයි කල්පනා කරමින් දරුවන් නිදාගත් පසු අවදියෙන් සිටින අම්මලා තාත්තලාද සිටිති.\n","A carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities. Some people try to reduce their carbon footprint because they are concerned about climate change.\tකාබන් පියසටහනක් යනු අපගේ ක්‍රියාකාරකම්වල ප්‍රතිඵලයක් ලෙස අප විසින් නිපදවන කාබන්ඩයොක්සයිඩ් දූෂණ ප්‍රමාණයයි. සමහර අය දේශගුණික විපර්යාස ගැන සැලකිලිමත් වන නිසා ඔවුන්ගේ කාබන් පියසටහන අඩු කිරීමට උත්සාහ කරති.\n","Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Google and hope for something less irritating.\tඕනෑම මාතෘකාවක සාමාන්‍යයෙන් වෙබ් පිටු කිහිපයක් ඇති බැවින්, මම සාමාන්‍යයෙන් උත්පතන දැන්වීම් ඇති වෙබ් පිටුවකට පැමිණෙන විට ආපසු බොත්තම ඔබන්නෙමි. මම සරලවම Google විසින් සොයා ගන්නා ලද ඊළඟ පිටුවට ගොස් අඩු කෝපයක් ඇති දෙයක් සොයා ගැනීමට බලාපොරොත්තු වෙමි.\n","If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\tඔබට ස්වදේශික කථිකයෙකු ලෙස ශබ්ද කිරීමට අවශ්‍ය නම්, බැන්ජෝ වාදකයෙකු එම වාක්‍ය ඛණ්ඩය නිවැරදිව හා නියමිත වේලාවට වාදනය කරන තෙක් එකම වාක්‍ය ඛණ්ඩය නැවත නැවතත් කීමට පුහුණු වීමට ඔබ කැමති විය යුතුය.\n","It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning, we might be able to minimize errors.\tමෙම ආකාරයේ සහයෝගී ප්‍රයත්නයේ ස්වභාවය නිසා සම්පූර්ණ දෝෂ රහිත corpus එකක් ලබා ගැනීමට නොහැකි විය හැක. කෙසේ වෙතත්, ඔවුන් ඉගෙන ගන්නා භාෂා සමඟ අත්හදා බැලීම් කරනවාට වඩා ඔවුන්ගේම භාෂාවෙන් වාක්‍ය ඛණ්ඩ දායක කිරීමට අපි සාමාජිකයින් දිරිමත් කරන්නේ නම්, අපට දෝෂ අවම කර ගැනීමට හැකි වනු ඇත.\n","One day, I woke up to find that God had put hair on my face. I shaved it off. The next day, I found that God had put it back on my face, so I shaved it off again. On the third day, when I found that God had put hair back on my face again, I decided to let God have his way. That's why I have a beard.\tදවසක් මම ඇහැරිලා බැලුවා දෙවියන් මගේ මූණට කෙස් ගහලා කියලා. මම ඒක රැවුල කැපුවා. ඊළඟ දවසේ, දෙවියන් වහන්සේ එය මගේ මුහුණට දමා ඇති බව මම දුටුවෙමි, මම එය නැවත රැවුල කපා ගත්තෙමි. තුන්වෙනි දවසේදී, දෙවියන් වහන්සේ නැවතත් මගේ මුහුණට හිසකෙස් දමා ඇති බව මම දුටු විට, මම දෙවියන් වහන්සේට ඔහුගේ මාර්ගයට ඉඩ දීමට තීරණය කළෙමි. ඒකයි මට රැවුල තියෙන්නේ.\n"]}]},{"cell_type":"markdown","source":["## **Split the English and Sinhala translation pairs**"],"metadata":{"id":"ucYoBXJ5O2AU"}},{"cell_type":"code","source":["text_pairs = []\n","for line in lines:\n","    english, sinhala = line.split(\"\\t\")\n","    sinhala = \"[start] \" + sinhala + \" [end]\"\n","    text_pairs.append((english, sinhala))\n","for i in range(3):\n","  print(random.choice(text_pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G2bJ5H_CO_Kk","executionInfo":{"status":"ok","timestamp":1713683511154,"user_tz":-330,"elapsed":820,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}},"outputId":"25836c40-c313-4b2e-cd7f-83a370df84cf"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["(\"Tom definitely knows that he shouldn't be doing that.\", '[start] ඔහු එසේ නොකළ යුතු බව ටොම් නිසැකවම දනී. [end]')\n","(\"He's looking good.\", '[start] ඔහු හොඳ පෙනුමක්. [end]')\n","('Tom asked me if I would like to cook.', '[start] ටොමස් මගෙන් ඇහුවා මට උයන්න ඕනද කියලා. [end]')\n"]}]},{"cell_type":"markdown","source":["## **Randomize the data**"],"metadata":{"id":"gsqhaL3zPFlV"}},{"cell_type":"code","source":["import random\n","random.shuffle(text_pairs)"],"metadata":{"id":"reVN_frpPLiF","executionInfo":{"status":"ok","timestamp":1713683515684,"user_tz":-330,"elapsed":2,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["## **Spliting the data into Training, Validation and Testing**"],"metadata":{"id":"2JkMrnwaPIfF"}},{"cell_type":"code","source":["num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples:]\n","print(\"Total sentences:\",len(text_pairs))\n","print(\"Training set size:\",len(train_pairs))\n","print(\"Validation set size:\",len(val_pairs))\n","print(\"Testing set size:\",len(test_pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iwQxiWNWPSbk","executionInfo":{"status":"ok","timestamp":1713683523224,"user_tz":-330,"elapsed":570,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}},"outputId":"38aeb0f4-b52e-415d-95fb-a3a90f464a93"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Total sentences: 125603\n","Training set size: 87923\n","Validation set size: 18840\n","Testing set size: 18840\n"]}]},{"cell_type":"code","source":["len(train_pairs)+len(val_pairs)+len(test_pairs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vcn5hvcgPjiu","executionInfo":{"status":"ok","timestamp":1713683531390,"user_tz":-330,"elapsed":632,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}},"outputId":"94c33558-a12a-48cb-94e6-077382c8c656"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["125603"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["## **Removing Punctuations**"],"metadata":{"id":"UViK0Xt7Po3U"}},{"cell_type":"code","source":["strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")"],"metadata":{"id":"h5wej2OrPtTE","executionInfo":{"status":"ok","timestamp":1713683534679,"user_tz":-330,"elapsed":2,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["f\"[{re.escape(strip_chars)}]\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Ty_SS80fPv1W","executionInfo":{"status":"ok","timestamp":1713683538827,"user_tz":-330,"elapsed":573,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}},"outputId":"1e31c1d6-f09c-4283-dc50-4a0233de0e54"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["f\"{3+5}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"JOrwIqdcPyWI","executionInfo":{"status":"ok","timestamp":1713683543694,"user_tz":-330,"elapsed":796,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}},"outputId":"986386fc-4d56-4afb-fd2f-3aba65ef0c8e"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'8'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["## **Vectorizing the English and Sinhala text pairs**"],"metadata":{"id":"YpVnaTnOP1s8"}},{"cell_type":"code","source":["def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(\n","        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n","vocab_size = 15000\n","sequence_length = 20\n","source_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length,\n",")\n","target_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_english_texts = [pair[0] for pair in train_pairs]\n","train_sinhala_texts = [pair[1] for pair in train_pairs]\n","source_vectorization.adapt(train_english_texts)\n","target_vectorization.adapt(train_sinhala_texts)"],"metadata":{"id":"bc4-BsIZP6E1","executionInfo":{"status":"ok","timestamp":1713683564406,"user_tz":-330,"elapsed":13533,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["## **Preparing datasets for the translation task**"],"metadata":{"id":"jWZbdRCOQACt"}},{"cell_type":"code","source":["batch_size = 64\n","def format_dataset(eng, spa):\n","    eng = source_vectorization(eng)\n","    spa = target_vectorization(spa)\n","    return ({\n","        \"english\": eng,\n","        \"sinhala\": spa[:, :-1],\n","    }, spa[:, 1:])\n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)\n","for inputs, targets in train_ds.take(1):\n","    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n","    print(f\"inputs['sinhala'].shape: {inputs['sinhala'].shape}\")\n","    print(f\"targets.shape: {targets.shape}\")\n","inputs['english'].shape: (64, 20)\n","inputs['sinhala'].shape: (64, 20)\n","targets.shape: (64, 20)\n","print(list(train_ds.as_numpy_iterator())[50])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbAROdF3QC6t","executionInfo":{"status":"ok","timestamp":1713683586189,"user_tz":-330,"elapsed":4277,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}},"outputId":"118a02c2-3487-4c0a-a746-f736408c6b5f"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs['english'].shape: (64, 20)\n","inputs['sinhala'].shape: (64, 20)\n","targets.shape: (64, 20)\n","({'english': array([[ 229,   67,  957, ...,    0,    0,    0],\n","       [  70,   68, 4232, ...,    0,    0,    0],\n","       [   3,  432,   52, ...,    0,    0,    0],\n","       ...,\n","       [  23,   46,    5, ...,    0,    0,    0],\n","       [  25,   82,   97, ...,    0,    0,    0],\n","       [   6, 7748,   10, ...,    0,    0,    0]]), 'sinhala': array([[    2,     5,    24, ...,     0,     0,     0],\n","       [    2,   455,   166, ...,     0,     0,     0],\n","       [    2,     4,   137, ...,     0,     0,     0],\n","       ...,\n","       [    2,     7,   465, ...,     0,     0,     0],\n","       [    2,    14,   290, ...,     0,     0,     0],\n","       [    2,     5, 11805, ...,     0,     0,     0]])}, array([[    5,    24,  6724, ...,     0,     0,     0],\n","       [  455,   166,    42, ...,     0,     0,     0],\n","       [    4,   137,   124, ...,     0,     0,     0],\n","       ...,\n","       [    7,   465,   192, ...,     0,     0,     0],\n","       [   14,   290,   246, ...,     0,     0,     0],\n","       [    5, 11805,   149, ...,     0,     0,     0]]))\n"]}]},{"cell_type":"markdown","source":["## **Transformer encoder implemented as a subclassed layer**"],"metadata":{"id":"J3ox9L8HQIcV"}},{"cell_type":"code","source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            mask = mask[:, tf.newaxis, :]\n","        attention_output = self.attention(\n","            inputs, inputs, attention_mask=mask)\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config"],"metadata":{"id":"pSa_t-0XQMcV","executionInfo":{"status":"ok","timestamp":1713683592524,"user_tz":-330,"elapsed":552,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["## **The Transformer decoder**"],"metadata":{"id":"EA5VamR9QRnM"}},{"cell_type":"code","source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1),\n","             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n","        return tf.tile(mask, mult)\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(\n","                mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","        else:\n","            padding_mask = mask\n","        attention_output_1 = self.attention_1(\n","            query=inputs,\n","            value=inputs,\n","            key=inputs,\n","            attention_mask=causal_mask)\n","        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n","        attention_output_2 = self.attention_2(\n","            query=attention_output_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        attention_output_2 = self.layernorm_2(\n","            attention_output_1 + attention_output_2)\n","        proj_output = self.dense_proj(attention_output_2)\n","        return self.layernorm_3(attention_output_2 + proj_output)"],"metadata":{"id":"gzaXDDhGQS78","executionInfo":{"status":"ok","timestamp":1713683608444,"user_tz":-330,"elapsed":555,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["## **Positional Encoding**"],"metadata":{"id":"umBJWBqoQX1d"}},{"cell_type":"code","source":["class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=input_dim, output_dim=output_dim)\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=output_dim)\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","    def get_config(self):\n","        config = super(PositionalEmbedding, self).get_config()\n","        config.update({\n","            \"output_dim\": self.output_dim,\n","            \"sequence_length\": self.sequence_length,\n","            \"input_dim\": self.input_dim,\n","        })\n","        return config"],"metadata":{"id":"DYj-J8ZHQZcd","executionInfo":{"status":"ok","timestamp":1713683617910,"user_tz":-330,"elapsed":784,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["## **End-to-End Transformer**"],"metadata":{"id":"mhamoXo4QcQ1"}},{"cell_type":"code","source":["embed_dim = 256\n","dense_dim = 2048\n","num_heads = 8\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"sinhala\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","transformer.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6MMFoNjeQe-F","executionInfo":{"status":"ok","timestamp":1713683642705,"user_tz":-330,"elapsed":1761,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}},"outputId":"2b12738a-c540-4f0f-f345-cbe47fc30b18"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," english (InputLayer)        [(None, None)]               0         []                            \n","                                                                                                  \n"," sinhala (InputLayer)        [(None, None)]               0         []                            \n","                                                                                                  \n"," positional_embedding_4 (Po  (None, None, 256)            3845120   ['english[0][0]']             \n"," sitionalEmbedding)                                                                               \n","                                                                                                  \n"," positional_embedding_5 (Po  (None, None, 256)            3845120   ['sinhala[0][0]']             \n"," sitionalEmbedding)                                                                               \n","                                                                                                  \n"," transformer_encoder_2 (Tra  (None, None, 256)            3155456   ['positional_embedding_4[0][0]\n"," nsformerEncoder)                                                   ']                            \n","                                                                                                  \n"," transformer_decoder_2 (Tra  (None, None, 256)            5259520   ['positional_embedding_5[0][0]\n"," nsformerDecoder)                                                   ',                            \n","                                                                     'transformer_encoder_2[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," dropout_2 (Dropout)         (None, None, 256)            0         ['transformer_decoder_2[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," dense_14 (Dense)            (None, None, 15000)          3855000   ['dropout_2[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 19960216 (76.14 MB)\n","Trainable params: 19960216 (76.14 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["## **Training the sequence-to-sequence Transformer**"],"metadata":{"id":"PXpOTSsJQqAe"}},{"cell_type":"code","source":["transformer.compile(\n","    optimizer=\"rmsprop\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"])\n","transformer.fit(train_ds, epochs=50, validation_data=val_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7cti6CBMQtT9","executionInfo":{"status":"ok","timestamp":1713689412198,"user_tz":-330,"elapsed":5739153,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}},"outputId":"34a0ffbb-d20c-4bbf-b385-87d201977809"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","1374/1374 [==============================] - 111s 76ms/step - loss: 4.3426 - accuracy: 0.4097 - val_loss: 3.4248 - val_accuracy: 0.5003\n","Epoch 2/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 3.3920 - accuracy: 0.5029 - val_loss: 3.0675 - val_accuracy: 0.5380\n","Epoch 3/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 3.0940 - accuracy: 0.5403 - val_loss: 2.9302 - val_accuracy: 0.5611\n","Epoch 4/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.9388 - accuracy: 0.5632 - val_loss: 2.8784 - val_accuracy: 0.5717\n","Epoch 5/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.8418 - accuracy: 0.5790 - val_loss: 2.8431 - val_accuracy: 0.5807\n","Epoch 6/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.7733 - accuracy: 0.5917 - val_loss: 2.8419 - val_accuracy: 0.5829\n","Epoch 7/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.7118 - accuracy: 0.6029 - val_loss: 2.8490 - val_accuracy: 0.5847\n","Epoch 8/50\n","1374/1374 [==============================] - 96s 70ms/step - loss: 2.6621 - accuracy: 0.6117 - val_loss: 2.8481 - val_accuracy: 0.5877\n","Epoch 9/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.6122 - accuracy: 0.6194 - val_loss: 2.8355 - val_accuracy: 0.5907\n","Epoch 10/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.5664 - accuracy: 0.6277 - val_loss: 2.8427 - val_accuracy: 0.5922\n","Epoch 11/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.5296 - accuracy: 0.6345 - val_loss: 2.8695 - val_accuracy: 0.5913\n","Epoch 12/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.4909 - accuracy: 0.6403 - val_loss: 2.8438 - val_accuracy: 0.5971\n","Epoch 13/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.4558 - accuracy: 0.6461 - val_loss: 2.8655 - val_accuracy: 0.5954\n","Epoch 14/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.4272 - accuracy: 0.6510 - val_loss: 2.8639 - val_accuracy: 0.5958\n","Epoch 15/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.4003 - accuracy: 0.6560 - val_loss: 2.8619 - val_accuracy: 0.5988\n","Epoch 16/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.3673 - accuracy: 0.6613 - val_loss: 2.8705 - val_accuracy: 0.6008\n","Epoch 17/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.3453 - accuracy: 0.6654 - val_loss: 2.8827 - val_accuracy: 0.5994\n","Epoch 18/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.3227 - accuracy: 0.6688 - val_loss: 2.8895 - val_accuracy: 0.6018\n","Epoch 19/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.2982 - accuracy: 0.6734 - val_loss: 2.9082 - val_accuracy: 0.6006\n","Epoch 20/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.2760 - accuracy: 0.6767 - val_loss: 2.9042 - val_accuracy: 0.6027\n","Epoch 21/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.2541 - accuracy: 0.6805 - val_loss: 2.9319 - val_accuracy: 0.6021\n","Epoch 22/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.2336 - accuracy: 0.6837 - val_loss: 2.9517 - val_accuracy: 0.6025\n","Epoch 23/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.2110 - accuracy: 0.6881 - val_loss: 2.9759 - val_accuracy: 0.6047\n","Epoch 24/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.1928 - accuracy: 0.6903 - val_loss: 2.9689 - val_accuracy: 0.6052\n","Epoch 25/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.1690 - accuracy: 0.6949 - val_loss: 2.9921 - val_accuracy: 0.6031\n","Epoch 26/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.1502 - accuracy: 0.6974 - val_loss: 3.0010 - val_accuracy: 0.6054\n","Epoch 27/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.1310 - accuracy: 0.7007 - val_loss: 3.0201 - val_accuracy: 0.6025\n","Epoch 28/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.1083 - accuracy: 0.7042 - val_loss: 3.0091 - val_accuracy: 0.6072\n","Epoch 29/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.0902 - accuracy: 0.7073 - val_loss: 3.0166 - val_accuracy: 0.6062\n","Epoch 30/50\n","1374/1374 [==============================] - 96s 70ms/step - loss: 2.0721 - accuracy: 0.7111 - val_loss: 3.0512 - val_accuracy: 0.6072\n","Epoch 31/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.0518 - accuracy: 0.7137 - val_loss: 3.0369 - val_accuracy: 0.6089\n","Epoch 32/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.0333 - accuracy: 0.7173 - val_loss: 3.0757 - val_accuracy: 0.6081\n","Epoch 33/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.0165 - accuracy: 0.7201 - val_loss: 3.0622 - val_accuracy: 0.6101\n","Epoch 34/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.9982 - accuracy: 0.7228 - val_loss: 3.0871 - val_accuracy: 0.6083\n","Epoch 35/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.9826 - accuracy: 0.7254 - val_loss: 3.0938 - val_accuracy: 0.6111\n","Epoch 36/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.9646 - accuracy: 0.7279 - val_loss: 3.1214 - val_accuracy: 0.6068\n","Epoch 37/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.9505 - accuracy: 0.7305 - val_loss: 3.1417 - val_accuracy: 0.6094\n","Epoch 38/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.9334 - accuracy: 0.7333 - val_loss: 3.1280 - val_accuracy: 0.6109\n","Epoch 39/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.9157 - accuracy: 0.7360 - val_loss: 3.1786 - val_accuracy: 0.6097\n","Epoch 40/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.9027 - accuracy: 0.7383 - val_loss: 3.1658 - val_accuracy: 0.6126\n","Epoch 41/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.8867 - accuracy: 0.7413 - val_loss: 3.1957 - val_accuracy: 0.6094\n","Epoch 42/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.8746 - accuracy: 0.7428 - val_loss: 3.2205 - val_accuracy: 0.6089\n","Epoch 43/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.8565 - accuracy: 0.7457 - val_loss: 3.2529 - val_accuracy: 0.6094\n","Epoch 44/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.8442 - accuracy: 0.7477 - val_loss: 3.2365 - val_accuracy: 0.6122\n","Epoch 45/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.8274 - accuracy: 0.7507 - val_loss: 3.2484 - val_accuracy: 0.6106\n","Epoch 46/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.8145 - accuracy: 0.7527 - val_loss: 3.2403 - val_accuracy: 0.6124\n","Epoch 47/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.8003 - accuracy: 0.7546 - val_loss: 3.2835 - val_accuracy: 0.6128\n","Epoch 48/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.7889 - accuracy: 0.7568 - val_loss: 3.2753 - val_accuracy: 0.6110\n","Epoch 49/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.7740 - accuracy: 0.7592 - val_loss: 3.3343 - val_accuracy: 0.6105\n","Epoch 50/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.7611 - accuracy: 0.7612 - val_loss: 3.3776 - val_accuracy: 0.6059\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7c280fff9d20>"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["## **Testing**"],"metadata":{"id":"hfHcYB8Se6ZW"}},{"cell_type":"code","source":["import numpy as np\n","spa_vocab = target_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = target_vectorization(\n","            [decoded_sentence])[:, :-1]\n","        predictions = transformer(\n","            [tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(20):\n","    input_sentence = random.choice(test_eng_texts)\n","    print(\"-\")\n","    print(input_sentence)\n","    print(decode_sequence(input_sentence))"],"metadata":{"id":"322GsnnSd15N","executionInfo":{"status":"ok","timestamp":1713692260948,"user_tz":-330,"elapsed":10240,"user":{"displayName":"sanjeewa sandeepani","userId":"00678955893741460314"}},"outputId":"8b54760a-f788-4ac4-9c02-bfa90dc482de","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["-\n","At least I haven't lost anything today.\n","[start] අවම වශයෙන් මම කිසිවක් අහිමි වී නැත [end]\n","-\n","He has hardly any money, but he gets by.\n","[start] එයාට සල්ලි නැති විදියට ගන්නවා නේද [end]\n","-\n","I have one.\n","[start] මට එකක් තියෙනවා [end]\n","-\n","Tom doesn't want to do his homework right now.\n","[start] ටොම්ට දැන් ගෙදර වැඩ කරන්න ඕන නෑ නේද [end]\n","-\n","He is cranky.\n","[start] ඔහු [UNK] ය [end]\n","-\n","It's difficult to learn a foreign language.\n","[start] මට පිටරට ඉගෙන ගන්න අමාරුයි [end]\n","-\n","Will it bother you if I turn on the radio?\n","[start] මම ගුවන් විදුලියට සවන් දෙන එක සකස් කිරීම ඔබට ස්තුතියි [end]\n","-\n","I only need one onion for this recipe.\n","[start] ඒ සඳහා [UNK] මට අවශ්‍ය වන්නේ [UNK] පමණයි [end]\n","-\n","Tom went straight to the post office.\n","[start] ටොම් තැපැල් ආච්චි තැපැල් සැමියා වෙත ගියේය [end]\n","-\n","Be careful what you wish for. It might come true.\n","[start] එය සිදු වූ දේ නොකිරීමට කලබල විය [end]\n","-\n","No news is good news.\n","[start] ඒක හොඳ වැඩක් නෑ [end]\n","-\n","Please keep on working even when I'm not here.\n","[start] කරුණාකර මෙහි රැඳී සිටින විට කිසිවෙකු වැඩ නොකරන්න [end]\n","-\n","I needed the money.\n","[start] මට මුදල් අවශ්‍ය විය [end]\n","-\n","Can I book two seats on that flight?\n","[start] මට මගේ ගුවන් රූපවාහිනිය නැරඹීමට ඉඩ දෙන්න පුළුවන්ද [end]\n","-\n","I'm not interested in doing that now.\n","[start] මම දැන් ඒ වගේ උනන්දු වෙන්න මම කරන්නේ නැහැ [end]\n","-\n","Maybe Tom didn't do what everyone says he did.\n","[start] සමහර විට ටොම් කියන දේ ඔහු කළ නොහැකි විය [end]\n","-\n","Our fridge is broken.\n","[start] අපේ [UNK] කඩා වැටුණි [end]\n","-\n","I'm dying of hunger.\n","[start] මම මැරිලා [end]\n","-\n","It must have been difficult for her to knit this sweater.\n","[start] ඒ අහම්බෙන් ඔහුගේ [UNK] [UNK] විය යුතුය [end]\n","-\n","He's a freshman.\n","[start] ඔහු [UNK] [end]\n"]}]}]}